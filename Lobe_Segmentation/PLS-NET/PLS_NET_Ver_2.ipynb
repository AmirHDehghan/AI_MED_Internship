{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PLS-NET-Ver_2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPUuvMSMQGcOHWdpHbz2oMK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmirHDehghan/AI_MED_Internship/blob/main/Lobe_Segmentation/PLS-NET/PLS_NET_Ver_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rt2LGmI0ePmv"
      },
      "source": [
        "#  Importing!\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from __future__ import print_function\n",
        "#%matplotlib inline\n",
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import HTML\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pylab as plt\n",
        "import nibabel as nib\n",
        "from skimage.transform import resize"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ofren4nceaOB",
        "outputId": "2a6580c9-c511-4b33-9173-dd9fb158ed6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Requird for reading mhd format\n",
        "!pip install SimpleITK\n",
        "import SimpleITK as sitk"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting SimpleITK\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/cb/a15f4612af8e37f3627fc7fb2f91d07bb584968b0a47e3d5103d7014f93e/SimpleITK-2.0.1-cp36-cp36m-manylinux1_x86_64.whl (44.9MB)\n",
            "\u001b[K     |████████████████████████████████| 44.9MB 69kB/s \n",
            "\u001b[?25hInstalling collected packages: SimpleITK\n",
            "Successfully installed SimpleITK-2.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YVOxnbhedtA",
        "outputId": "2039ad54-96fb-4cec-cd11-ccf419965fc0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Reading 3 samples from VESSEL12 dataset\n",
        "!wget https://www.dropbox.com/sh/1j2x17k8y18k3l6/AACkAc6bCEqYVXBdODFo6Iqya/VESSEL12_ExampleScans.tar.bz2"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-31 10:29:11--  https://www.dropbox.com/sh/1j2x17k8y18k3l6/AACkAc6bCEqYVXBdODFo6Iqya/VESSEL12_ExampleScans.tar.bz2\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.6.1, 2620:100:601c:1::a27d:601\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.6.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /sh/raw/1j2x17k8y18k3l6/AACkAc6bCEqYVXBdODFo6Iqya/VESSEL12_ExampleScans.tar.bz2 [following]\n",
            "--2020-10-31 10:29:12--  https://www.dropbox.com/sh/raw/1j2x17k8y18k3l6/AACkAc6bCEqYVXBdODFo6Iqya/VESSEL12_ExampleScans.tar.bz2\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc4de42f2c0003a75e26276575bc.dl.dropboxusercontent.com/cd/0/inline/BCTdt9hEV_lfeKJTudN8kgqG5ino_L4YNVeEaFnzXWYkzdr09M0_R5FZLM6ZZweS1ScAV6C38ZCkUtKK8pa476a_PiP-IiGo0AY4bERKbTRTkw/file# [following]\n",
            "--2020-10-31 10:29:12--  https://uc4de42f2c0003a75e26276575bc.dl.dropboxusercontent.com/cd/0/inline/BCTdt9hEV_lfeKJTudN8kgqG5ino_L4YNVeEaFnzXWYkzdr09M0_R5FZLM6ZZweS1ScAV6C38ZCkUtKK8pa476a_PiP-IiGo0AY4bERKbTRTkw/file\n",
            "Resolving uc4de42f2c0003a75e26276575bc.dl.dropboxusercontent.com (uc4de42f2c0003a75e26276575bc.dl.dropboxusercontent.com)... 162.125.6.15, 2620:100:601c:15::a27d:60f\n",
            "Connecting to uc4de42f2c0003a75e26276575bc.dl.dropboxusercontent.com (uc4de42f2c0003a75e26276575bc.dl.dropboxusercontent.com)|162.125.6.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/BCT_GQdtIj8eYUqM88IjY6d0U8AQk0f51HmHfP8MSWNtVWgVnfhDkvU10AjgtLre-xDPL5BdMJedCY3X-asaLTV0edBRTA23YdsmEElVijILHvMnAu3iur89pPXr00J7oRlOBpkR1irMy3X1GQdU-v0ll_2iP-qDJABLbyaL51H0M8klQvKH-eCHCzF6BuHkp4XMSZg6ejAztYP_iBfMt3aXUEwc-LghtsBTQ9dS_jq7tBFrYzKz0fJysSbP_fflyzDz-qgXBgioyTFzPRku6D3sQ-Yfc03aMIcGbh6jLZ2v61DDo1CT-eQVryshh375esZlrMBxO9acD4dZMMnh3rgD/file [following]\n",
            "--2020-10-31 10:29:13--  https://uc4de42f2c0003a75e26276575bc.dl.dropboxusercontent.com/cd/0/inline2/BCT_GQdtIj8eYUqM88IjY6d0U8AQk0f51HmHfP8MSWNtVWgVnfhDkvU10AjgtLre-xDPL5BdMJedCY3X-asaLTV0edBRTA23YdsmEElVijILHvMnAu3iur89pPXr00J7oRlOBpkR1irMy3X1GQdU-v0ll_2iP-qDJABLbyaL51H0M8klQvKH-eCHCzF6BuHkp4XMSZg6ejAztYP_iBfMt3aXUEwc-LghtsBTQ9dS_jq7tBFrYzKz0fJysSbP_fflyzDz-qgXBgioyTFzPRku6D3sQ-Yfc03aMIcGbh6jLZ2v61DDo1CT-eQVryshh375esZlrMBxO9acD4dZMMnh3rgD/file\n",
            "Reusing existing connection to uc4de42f2c0003a75e26276575bc.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 313639176 (299M) [application/octet-stream]\n",
            "Saving to: ‘VESSEL12_ExampleScans.tar.bz2’\n",
            "\n",
            "VESSEL12_ExampleSca 100%[===================>] 299.11M  62.9MB/s    in 4.7s    \n",
            "\n",
            "2020-10-31 10:29:18 (63.8 MB/s) - ‘VESSEL12_ExampleScans.tar.bz2’ saved [313639176/313639176]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQUlpJhPegK2"
      },
      "source": [
        "#Unzip samples\n",
        "!tar xf VESSEL12_ExampleScans.tar.bz2"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qpr1HAWnejIR"
      },
      "source": [
        "def read_mhd(file):\n",
        "    return sitk.GetArrayFromImage(sitk.ReadImage(file, sitk.sitkFloat32))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7S4vfmCpelWt",
        "outputId": "75d7c556-e274-42e5-9441-50206b91e38b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "vessel12_21 = read_mhd(\"Scans/VESSEL12_21.mhd\")\n",
        "vessel12_21.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(459, 512, 512)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMbmW5D-enkM",
        "outputId": "52b6e466-4cbb-43e8-d14e-4f07dc548de3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "vessel12_21_mask = read_mhd(\"Lungmasks/VESSEL12_21.mhd\")\n",
        "vessel12_21_mask.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(459, 512, 512)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6wZrxsdep0W"
      },
      "source": [
        "def draw(images, columns=4):\n",
        "    rows = int(np.ceil(images.shape[0] / columns))\n",
        "    max_size = 20\n",
        "    \n",
        "    width = max(columns * 5, max_size)\n",
        "    height = width * rows // columns\n",
        "\n",
        "    plt.figure(figsize=(width, height))\n",
        "    plt.gray()\n",
        "    plt.subplots_adjust(0,0,1,1,0.01,0.01)\n",
        "    for i in range(images.shape[0]):\n",
        "        plt.subplot(rows,columns,i+1), plt.imshow(images[i]), plt.axis('off')\n",
        "        # use plt.savefig(...) here if you want to save the images as .jpg, e.g.,\n",
        "    plt.show()\n",
        "\n",
        "def draw_masked(images, masks, columns=4):\n",
        "    assert images.shape == masks.shape\n",
        "    \n",
        "    rows = int(np.ceil(images.shape[0] / columns))\n",
        "    max_size = 20\n",
        "    \n",
        "    width = min(columns * 5, max_size)\n",
        "    height = width * rows // columns\n",
        "\n",
        "    fig = plt.figure(figsize=(width, height))\n",
        "    plt.gray()\n",
        "    plt.subplots_adjust(0,0,1,1,0.01,0.01)\n",
        "    \n",
        "    X, Y = np.meshgrid(np.arange(masks.shape[1]), np.arange(masks.shape[2]))\n",
        "    \n",
        "    for i in range(images.shape[0]):\n",
        "        ax = fig.add_subplot(rows,columns,i+1)\n",
        "        if masks[i].sum() > 0:\n",
        "            ax.contour(X, Y, masks[i], 1, colors='red', linewidths=0.5)\n",
        "        ax.imshow(images[i], origin='lower', cmap='gray')\n",
        "        plt.axis('off')\n",
        "    \n",
        "    plt.show()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sq2yT5YHer8p"
      },
      "source": [
        "# custom weights initialization\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.01)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.01)\n",
        "        nn.init.constant_(m.bias.data, 0)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITK5VjD7ev4t"
      },
      "source": [
        "# class Encoder_DSconv(nn.Module):\n",
        "#   def __init__(self, in_channels, out_channels, DSkernel, DSstride, DSpadding):\n",
        "#     super(Encoder_DSconv, self).__init__()\n",
        "\n",
        "#     self.dsconv = nn.Sequential(\n",
        "#         #Input is 10, 512, 512\n",
        "\n",
        "#         nn.Conv3d(in_channels, in_channels, DSkernel, 2, DSpadding, groups = in_channels),\n",
        "\n",
        "#         nn.BatchNorm3d(in_channels),\n",
        "#         nn.ReLU(inplace=True),\n",
        "#         nn.Conv3d(in_channels, out_channels, 1 ,DSstride, 0),\n",
        "#         nn.BatchNorm3d(out_channels),\n",
        "#         nn.ReLU(inplace=True)\n",
        "#     )\n",
        "\n",
        "#   def forward(self, input):\n",
        "#     # print(\"dsconv shape of input:\", input.shape)  \n",
        "#     return self.dsconv(input)\n",
        "\n",
        "class DSconv(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, DSkernel, depth_stride, DSstride, DSpadding, dilation=1):\n",
        "    super(DSconv, self).__init__()\n",
        "\n",
        "    self.dsconv = nn.Sequential(\n",
        "        #Input is 10, 512, 512\n",
        "\n",
        "        nn.Conv3d(in_channels, in_channels, DSkernel, depth_stride, DSpadding, dilation, groups = in_channels),\n",
        "\n",
        "        nn.BatchNorm3d(in_channels),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv3d(in_channels, out_channels, 1 ,DSstride, 0, dilation),\n",
        "        nn.BatchNorm3d(out_channels),\n",
        "        nn.ReLU(inplace=True)\n",
        "    )\n",
        "\n",
        "  def forward(self, input):\n",
        "    # print(\"dsconv shape of input:\", input.shape)  \n",
        "    return self.dsconv(input)\n",
        "\n",
        "\n",
        "class DRDB(nn.Module):\n",
        "      def __init__(self, in_channels, DRDBpadding):\n",
        "        super(DRDB, self).__init__()\n",
        "        \n",
        "        DSkernel = 3\n",
        "        DSstride = 1\n",
        "        DSpadding = DRDBpadding\n",
        "        self.conv_1 = DSconv(in_channels, in_channels, 3, 1, DSstride, 1, 1)\n",
        "        self.conv_2 = DSconv(in_channels * 2 , in_channels * 2, 3, 1, DSstride, 2, 2)\n",
        "        self.conv_3 = DSconv(in_channels * 4, in_channels * 4, 3, 1, DSstride, 3, 3)\n",
        "        self.conv_4 = DSconv(in_channels * 8, in_channels * 8, 3, 1, DSstride, 4, 4)\n",
        "        \n",
        "        self.final_conv = nn.Conv3d(in_channels * 16, in_channels, 1, DSstride, 0,bias=False)\n",
        "\n",
        "      def forward(self, DRDBinput):\n",
        "        first_convolved = self.conv_1(DRDBinput)\n",
        "        # print(first_convolved.shape)\n",
        "        concat = torch.cat([first_convolved, DRDBinput], dim=1)\n",
        "        second_convolved = self.conv_2(concat)\n",
        "        # print(\"Testing\")\n",
        "        # print(second_convolved.shape)\n",
        "        # print(DRDBinput.shape)\n",
        "        concat = torch.cat([second_convolved, concat], dim=1)\n",
        "        third_convolved = self.conv_3(concat)\n",
        "        concat = torch.cat([third_convolved, concat], dim=1)\n",
        "        forth_convolved = self.conv_4(concat)\n",
        "        concat = torch.cat([forth_convolved, concat], dim=1)\n",
        "        xg0 = self.final_conv(concat)\n",
        "        # fff = xg0 + DRDBinput\n",
        "        # print(fff.shape)\n",
        "        return xg0 + DRDBinput\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytXb2mA3ewps"
      },
      "source": [
        "test = torch.zeros(size= [1, 17, 10, 512, 512], device = 'cuda:0', dtype = torch.float16)\n",
        "drdb = DRDB(17, 0)\n",
        "drdb = drdb.to('cuda:0').half()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_-_TQYdezLa",
        "outputId": "1c2d7f1d-6e17-4073-fc71-a874b2c2789a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "out = drdb(test)\n",
        "out.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 17, 10, 512, 512])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsLOEu-me1u1",
        "outputId": "8387b370-ba78-4e5f-c776-fe08126535a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "import gc\n",
        "gc.collect()"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "760"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lImBSDoe350",
        "outputId": "d5d48f5d-1e7f-40a5-da71-73ecffa969e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "test = torch.zeros(size= [1, 1, 8, 512, 512], device = 'cuda:0', dtype = torch.float16)\n",
        "dsconv = nn.Sequential( DSconv(1, 16, 3, 2, 1, 1, dilation = 1),\n",
        "DSconv(16, 16, 3, 2, 1, 1, dilation = 1),\n",
        "DSconv(16, 1, 3, 2, 1, 1, dilation = 1))\n",
        "dsconv = dsconv.to('cuda:0').half()\n",
        "out = dsconv(test)\n",
        "out.shape"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1, 1, 64, 64])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HH9A1blAe6eS"
      },
      "source": [
        "class PLSnet(nn.Module):\n",
        "  def __init__(self, DSkernel, DSstride, DSpadding, C):\n",
        "    super(PLSnet, self).__init__()\n",
        "\n",
        "    self.dsconvolve_1 = DSconv(1, 16, DSkernel, 2, DSstride, DSpadding)\n",
        "    self.dsconvolve_2 = DSconv(17, 64, DSkernel, 2, DSstride, DSpadding)\n",
        "    self.dsconvolve_3 = DSconv(65, 128, DSkernel, 2, DSstride, DSpadding)\n",
        "\n",
        "    self.mid_dsconvolve_1 = DSconv(17, 2*C, DSkernel, 1, DSstride, DSpadding, 1)\n",
        "    self.mid_dsconvolve_2 = DSconv(65, 2*C, DSkernel, 1, DSstride, DSpadding, 1)\n",
        "    \n",
        "    self.decoder_dsconvolve_1 = DSconv(129, 2*C, DSkernel, 1, DSstride, DSpadding, 1)\n",
        "    self.decoder_dsconvolve_2 = DSconv(4*C, 2*C, DSkernel, 1, DSstride, DSpadding, 1)\n",
        "    self.decoder_dsconvolve_3 = DSconv(4*C, 2*C, DSkernel, 1, DSstride, DSpadding, 1)\n",
        "\n",
        "    self.oneconv = nn.Conv3d(2*C, C, 1 ,DSstride, 0)\n",
        "\n",
        "    DRDBpadding = DSpadding\n",
        "    self.drdb = DRDB(17, DRDBpadding)\n",
        "    self.drdbx2 = DRDB(65, DRDBpadding)\n",
        "    self.drdbx4 = DRDB(129, DRDBpadding)\n",
        "\n",
        "    self.TLupsample = nn.Upsample(scale_factor=2, mode='trilinear')\n",
        "    self.softmaxAF = nn.Softmax()\n",
        "    self.loss = nn.CrossEntropyLoss()\n",
        "\n",
        "  def forward(self, InputImage, real_mask):\n",
        "    # resolution = 1 in encoder\n",
        "    DSconv_output = self.dsconvolve_1(InputImage)\n",
        "    # print(\"dsconv: \", DSconv_1.shape)\n",
        "    InputImage = nn.functional.interpolate(InputImage, scale_factor=1/2, mode='trilinear')\n",
        "    # print(\"image: \", InputImage.shape)\n",
        "    concat_output = torch.cat([DSconv_output,InputImage], dim=1)\n",
        "    DRDB1_output = self.drdb(concat_output)\n",
        "    # resolution = 2 in encoder\n",
        "    DSconv_output = self.dsconvolve_2(DRDB1_output)\n",
        "    InputImage = nn.functional.interpolate(InputImage, scale_factor=1/2, mode='trilinear')\n",
        "    concat_output = torch.cat([DSconv_output,InputImage], dim=1)\n",
        "    DRDB2_output = self.drdbx2(self.drdbx2(concat_output))\n",
        "    # resolution = 3 in encoder\n",
        "    DSconv_output = self.dsconvolve_3(DRDB2_output)\n",
        "\n",
        "    InputImage = nn.functional.interpolate(InputImage, scale_factor=1/2, mode = 'trilinear')\n",
        "    concat_output = torch.cat([DSconv_output,InputImage], dim=1)\n",
        "\n",
        "    DSconv_output = self.decoder_dsconvolve_1(self.drdbx4(self.drdbx4(self.drdbx4(self.drdbx4(concat_output)))))\n",
        "    upsample_output = self.TLupsample(DSconv_output) \n",
        "    # resolution = 2 in decoder\n",
        "    DSconv_output = self.mid_dsconvolve_2(DRDB2_output)\n",
        "    concat_output = torch.cat([upsample_output,DSconv_output], dim = 1)\n",
        "    DSconv_output = self.decoder_dsconvolve_2(concat_output)\n",
        "    upsample_output = self.TLupsample(DSconv_output)\n",
        "    # resolution = 1 in decoder\n",
        "    DSconv_output = self.mid_dsconvolve_1(DRDB1_output)\n",
        "    concat_output = torch.cat([upsample_output,DSconv_output], dim = 1)\n",
        "    DSconv_output = self.decoder_dsconvolve_2(concat_output)\n",
        "    upsample_output = self.TLupsample(DSconv_output)\n",
        "    # resolution = 0 in decoder\n",
        "    output = self.oneconv(upsample_output)\n",
        "    predicted = self.softmaxAF(output)\n",
        "    # print(output.shape,real_mask.shape)\n",
        "    loss_output = self.loss(output,real_mask.squeeze(dim=1))\n",
        "\n",
        "    return predicted, loss_output"
      ],
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXZmo1NHfBHh"
      },
      "source": [
        "# in_channels = 10\n",
        "# out_channels = 256\n",
        "DSkernel = 3\n",
        "DSstride = 1\n",
        "DSpadding = 1\n",
        "# g = 12\n",
        "# g0 = 17\n",
        "C = 6   #number of classes\n",
        "\n",
        "model = PLSnet(DSkernel, DSstride, DSpadding, C)\n",
        "model = model.cuda().half()"
      ],
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GL82GBPfD8r"
      },
      "source": [
        "optimizer = torch.optim.Adam(filter(\n",
        "    lambda p : p.requires_grad, model.parameters()),\n",
        "    lr = 0.001\n",
        ")"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHXu6wm5fHv3"
      },
      "source": [
        "batch_size = 2\n",
        "batch_x_placeholder = torch.zeros(size= [batch_size, 1, 8, 512, 512], dtype = torch.float16, device = 'cuda:0')\n",
        "batch_y_placeholder = torch.zeros(size= [batch_size, 1, 8, 512, 512], dtype = torch.long, device = 'cuda:0')\n",
        "\n",
        "x_train = vessel12_21[200:208]\n",
        "y_train = vessel12_21_mask[200:208]\n",
        "\n",
        "# print(y_train.shape)\n",
        "train_loss = np.zeros((batch_size*10,))\n",
        "val_loss = np.zeros((batch_size*10,))\n",
        "train_acc = np.zeros((batch_size*10,))\n",
        "val_acc = np.zeros((batch_size*10,))\n",
        "epochs = 10"
      ],
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwnX7cW9fIpP"
      },
      "source": [
        "from time import time"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LlLq1T0ffQ8",
        "outputId": "af9bb074-7ece-40f0-82b9-131ba7b6d312",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "inds = np.arange(len(x_train))\n",
        "len(y_train)"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEDxVzxDfK7F",
        "outputId": "559edf70-f2d0-4754-ee08-cadfaf4e9de4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "iters_per_epoch = int(np.ceil(1.0 * len(x_train) / batch_size))\n",
        "\n",
        "for e in range(epochs):\n",
        "    t_start = time()\n",
        "\n",
        "    model.train() # training phase\n",
        "\n",
        "    # shuffling\n",
        "    inds = np.arange(len(x_train))\n",
        "    np.random.shuffle(inds)\n",
        "\n",
        "    epoch_loss = 0\n",
        "    true_positive = 0\n",
        "\n",
        "\n",
        "    # iterating over the whole training set\n",
        "    for iter in range(iters_per_epoch):\n",
        "\n",
        "        batch_inds = inds[iter * batch_size: min(len(inds), (iter + 1) * batch_size)]\n",
        "        # print(batch_inds)\n",
        "\n",
        "        # reshaping placeholders\n",
        "        if len(batch_inds) != len(batch_x_placeholder):\n",
        "            batch_x_placeholder.resize_([len(batch_inds), 1, 8, 512, 512])\n",
        "            batch_y_placeholder.resize_([len(batch_inds), 1, 8, 512, 512])\n",
        "\n",
        "        # print(\"HERE\")\n",
        "        # print(y_train.shape)\n",
        "        # print(batch_y_placeholder.shape)\n",
        "        batch_x_placeholder.copy_(torch.Tensor(x_train[batch_inds, np.newaxis, np.newaxis, :, :]))\n",
        "\n",
        "        batch_y_placeholder.copy_(torch.Tensor(y_train[batch_inds, np.newaxis, np.newaxis, :, :].astype(int)))\n",
        "\n",
        "        \n",
        "        b_decision, b_loss = model(batch_x_placeholder, batch_y_placeholder)\n",
        "        b_decision = b_decision.detach().cpu().numpy()\n",
        "      \n",
        "        epoch_loss += float(b_loss) / iters_per_epoch  # CARE: WE SHOULD USE FLOAT OVER LOSS\n",
        "        true_positive += np.sum(y_train[batch_inds[0]].astype(int) == b_decision)\n",
        "\n",
        "        b_loss.backward() # calculates derivations\n",
        "\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad() # CARE: MUST DO\n",
        "\n",
        "    epoch_train_accuracy = true_positive * 100.0 / len(x_train)\n",
        "    train_loss[e] = epoch_loss\n",
        "    train_acc[e] = epoch_train_accuracy\n",
        "    \n",
        " # Validating over validation data\n",
        "    # with torch.no_grad():\n",
        "    #     model.eval()  # validation phase\n",
        "\n",
        "    #     val_inds = np.arange(len(x_val))\n",
        "\n",
        "    #     val_iters_per_epoch = int(np.ceil(1.0 * len(x_val) / batch_size))\n",
        "\n",
        "    #     epoch_validation_loss = 0\n",
        "    #     val_true_positive = 0\n",
        "\n",
        "\n",
        "    #     # iterating over the whole training set\n",
        "    #     for iter in range(val_iters_per_epoch):\n",
        "\n",
        "    #         val_batch_inds = val_inds[iter * batch_size: min(len(val_inds), (iter + 1) * batch_size)]\n",
        "\n",
        "    #         # reshaping placeholders\n",
        "    #         if len(batch_inds) != len(batch_x_placeholder):\n",
        "    #             batch_x_placeholder.resize_([len(batch_inds), 1, 8, 512, 512])\n",
        "    #             batch_y_placeholder.resize_([len(batch_inds)])\n",
        "\n",
        "    #         batch_x_placeholder.copy_(torch.Tensor(x_train[batch_inds, np.newaxis, :, :, :]))\n",
        "    #         batch_y_placeholder.copy_(torch.Tensor(y_train[batch_inds].astype(int)))\n",
        "\n",
        "    #         b_decision, b_loss = model(batch_x_placeholder, batch_y_placeholder)\n",
        "    #         b_decision = b_decision.cpu().numpy()\n",
        "        \n",
        "    #         epoch_validation_loss += float(b_loss) / val_iters_per_epoch  # CARE: WE SHOULD USE FLOAT OVER LOSS\n",
        "        #     val_true_positive += np.sum(y_val[val_batch_inds].astype(int) == b_decision)\n",
        "                \n",
        "        # epoch_validation_accuracy = val_true_positive * 100.0 / len(x_val)\n",
        "        # val_loss[e] = epoch_validation_loss\n",
        "        # val_acc[e] = epoch_validation_accuracy\n",
        "        # TO Complete\n",
        "    \n",
        "    # print(f'Train epoch Loss: {epoch_loss:.4f}, train accuracy: {epoch_train_accuracy:.2f}, Validation Loss: {epoch_validation_loss:.4f}, validation accuracy: {epoch_validation_accuracy:.2f}')\n",
        "\n",
        "\n",
        "    print(f'Train epoch Loss: {epoch_loss:.4f}, train accuracy: {epoch_train_accuracy:.2f}')\n",
        "\n",
        "    # Saving the model and optimizer state\n",
        "    torch.save({\n",
        "            'epoch': e,\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'train_loss': epoch_loss,\n",
        "            'train_accuracy': epoch_train_accuracy,\n",
        "            # 'validation_loss': epoch_validation_loss,\n",
        "            # 'validation_accuracy': epoch_validation_accuracy\n",
        "        }, 'epoch_%d_state.pt' % e)\n",
        "    \n",
        "    \n",
        "\n",
        "    print('Epoch %d ended in %.2f secs.' % (e, time() - t_start,))\n"
      ],
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3121: UserWarning: Default upsampling behavior when mode=trilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode))\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3000: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and uses scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:60: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train epoch Loss: nan, train accuracy: 0.00\n",
            "Epoch 0 ended in 17.35 secs.\n",
            "Train epoch Loss: nan, train accuracy: 0.00\n",
            "Epoch 1 ended in 17.32 secs.\n",
            "Train epoch Loss: nan, train accuracy: 0.00\n",
            "Epoch 2 ended in 17.40 secs.\n",
            "Train epoch Loss: nan, train accuracy: 0.00\n",
            "Epoch 3 ended in 17.47 secs.\n",
            "Train epoch Loss: nan, train accuracy: 0.00\n",
            "Epoch 4 ended in 17.55 secs.\n",
            "Train epoch Loss: nan, train accuracy: 0.00\n",
            "Epoch 5 ended in 17.60 secs.\n",
            "Train epoch Loss: nan, train accuracy: 0.00\n",
            "Epoch 6 ended in 17.64 secs.\n",
            "Train epoch Loss: nan, train accuracy: 0.00\n",
            "Epoch 7 ended in 17.67 secs.\n",
            "Train epoch Loss: nan, train accuracy: 0.00\n",
            "Epoch 8 ended in 17.73 secs.\n",
            "Train epoch Loss: nan, train accuracy: 0.00\n",
            "Epoch 9 ended in 17.73 secs.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8-CTJz4fNdj"
      },
      "source": [
        "torch.cuda.reset_max_memory_allocated"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}